---
author: Hazuki Keatsu
pubDatetime: 2025-09-01T17:40:18+08:00
title: 突破不完整多模态学习瓶颈：MoMKE框架的创新与实践
featured: false
draft: true
tags:
  - 人工智能
  - 论文阅读
description: 模态知识专家混合模型（MoMKE）尝试解决了，在多模态情感识别（MER）领域，由于传感器损坏、隐私保护等现实问题常导致模态数据不完整的情景下的，因数据填补质量不稳定、计算成本高难以满足实时需求，和因忽视单模态表示学习，在单模态等严重不完整场景下性能拉胯的问题。
---

## 前言

在多模态情感识别（MER）领域，传感器损坏、隐私保护等现实问题常导致模态数据不完整，而现有方法在这类场景下，要么因数据填补质量不稳定、计算成本高难以满足实时需求，要么因忽视单模态表示学习，在单模态等严重不完整场景下性能拉胯。近期，来自西安电子科技大学的研究团队提出了“模态知识专家混合模型（MoMKE）”，为解决这些难题提供了新思路，接下来我们就深入剖析这一创新框架。

> [!INFO]
> 论文标题：Leveraging Knowledge of Modality Experts for Incomplete Multimodal Learning
>
> 发布时间：2024
>
> 引用次数：7
>
> 论文来源：Proceedings of the 32nd ACM International Conference on Multimedia
>
> 代码仓库：http://github.com/wxxv/MoMKE

## 一、MoMKE框架：核心设计与原理
MoMKE的核心突破在于**明确区分并同时学习强判别性单模态表示与跨模态一致的联合表示**，通过两阶段训练机制与动态融合策略，实现对不完整多模态场景的鲁棒适应。

### 1. 两阶段训练：从“专家专精”到“协同融合”
MoMKE的训练过程分为两个关键阶段，形成“分而治之、再融合增效”的逻辑闭环：
- **单模态专家训练阶段**：为音频、文本、视觉三种模态分别构建“专家模型”。每个专家仅使用对应模态的数据独立训练，通过预训练模型（如音频用wav2vec、文本用DeBERTa、视觉用MA-Net）结合全连接层，专注学习单模态特有的判别性知识。例如，音频专家重点捕捉语气、语速中的情感线索，文本专家则聚焦语义情感表达，确保每个模态的“专属能力”足够扎实。
- **专家混合训练阶段**：在单模态专家基础上，引入“软路由（Soft Router）”动态融合单模态表示与联合表示。软路由通过两层MLP网络，根据输入特征自动计算不同专家输出的权重，实现“按需融合”——既保留单模态的强判别性，又通过其他模态专家的知识补充联合表示的跨模态一致性。比如在仅音频可用的场景中，文本和视觉专家的输出虽为零向量，但联合表示仍能融入其学习到的通用情感规律，提升识别准确性。

### 2. 核心公式：量化模态表示融合逻辑
为清晰定义模态表示的融合过程，MoMKE提出关键公式：
- 单模态表示：$R_{uni}^m = e^m(X_m)$（$e^m$为第m个模态专家，$X_m$为模态特征）
- 联合表示：$R_{joint}^M = e^M(X_M)$（$e^M$为多专家协同模型，$X_M$为多模态特征集）
- 最终融合表示：$R = \alpha R_{uni} + \beta R_{joint}$（$\alpha$、$\beta$由软路由动态生成，根据输入场景自适应调整单模态与联合表示的贡献比例）

这一公式打破了传统方法“重联合、轻单模态”的局限，通过权重动态分配，让模型在不同完整性场景下都能发挥最优性能。

## 二、实验验证：性能与鲁棒性双优
为验证MoMKE的有效性，研究团队在三大主流多模态情感数据集（IEMOCAP、CMU-MOSI、CMU-MOSEI）上开展对比实验，并设计消融实验拆解关键模块的贡献。

### 1. 数据集与评估指标
| 数据集 | 数据规模与场景 | 评估指标 |
| ---- | ---- | ---- |
| IEMOCAP | 5组双人对话，四分类情感（快乐、悲伤、中性、愤怒） | 加权准确率（WA）、未加权准确率（UA） |
| CMU-MOSI | 2199个YouTube视频片段，情感分数[-3,3] | 分类准确率（ACC）、F1分数 |
| CMU-MOSEI | 22856个YouTube视频片段，覆盖1000+发言者 | 分类准确率（ACC）、F1分数 |

### 2. 对比实验：全面超越SOTA方法
在不同模态完整性场景下，MoMKE均展现出显著优势：
- **严重不完整场景（单模态）**：在IEMOCAP的仅音频场景中，MoMKE的WA/UA达到70.32%/71.38%，较现有最优方法（MRAN）提升13.74%/12.38%；仅文本场景下，WA/UA达77.82%/78.37%，领先第二名10.80%/10.17%。
- **部分完整场景（双模态）**：如IEMOCAP的“音频+视觉”场景，MoMKE的WA/UA为68.85%/67.65%，仍保持5%以上的性能优势。
- **完整模态场景**：在三大数据集的完整模态测试中，MoMKE的各项指标均为最优，证明其在充分数据条件下同样具备强大能力。

唯一例外是CMU-MOSI的仅音频场景，MoMKE的F1分数未达最优，研究者分析是该数据集规模较小（仅2199个样本），导致模型轻微过拟合，后续可通过数据增强进一步优化。

### 3. 消融实验：关键模块不可或缺
为验证两阶段训练与软路由的必要性，研究团队设计三组消融实验：
- **移除单模态专家训练**：所有数据集的指标平均下降3%-5%，证明单模态专家学习的判别性知识是性能基石。
- **移除专家混合训练**：模型在单模态场景下性能暴跌，如IEMOCAP仅视觉场景的WA/UA从58.60%/54.70%降至56.87%/53.37%，说明联合表示对补充单模态信息至关重要。
- **移除软路由（权重平均）**：性能虽优于前两组消融实验，但仍比完整MoMKE低2%-3%，凸显动态权重分配的价值——固定权重无法适配不同场景的模态需求。

此外，专家组合实验还发现：使用“对应模态专家+其他模态专家”的组合，性能显著优于仅使用对应模态专家，进一步证实“单模态+联合表示”融合的合理性。

## 三、深度解析：可视化与案例佐证
### 1. 专家负载可视化：动态适配的直观体现
通过跟踪训练过程中软路由分配的专家权重，研究团队发现：在单模态场景下，模型会自动提高对应模态专家的权重（如仅音频场景中，音频专家权重稳定在0.6-0.7），同时保留20%-30%的权重给其他模态专家的联合表示。这一结果验证了MoMKE“按需融合”的设计逻辑——既依赖单模态的核心信息，又不忽视跨模态的辅助规律。

### 2. 案例学习：像人类一样“补全”情感线索
在IEMOCAP的仅文本案例中，一段从“悲伤”到“中性”再到“愤怒”的对话，仅使用文本专家时，模型多次将“愤怒”误判为“中性”；而MoMKE通过融合音频、视觉专家的联合表示（虽无实际数据，但专家已学习到“激烈语义对应高情绪强度”的规律），准确捕捉到情绪变化，推理结果与人工标注完全一致。这一案例生动说明：MoMKE能像人类一样，通过文本语义“想象”语气、表情等缺失模态的线索，实现更全面的情感理解。

## 四、总结与展望：突破与局限并存
### 1. 核心贡献
- **方法论创新**：首次明确区分单模态表示与联合表示的学习，填补现有方法在单模态场景下的性能空白。
- **工程价值**：两阶段训练机制降低了模型训练难度，软路由的轻量化设计（两层MLP）确保实时性，可应用于传感器不稳定、隐私保护要求高的场景（如车载情感交互、远程心理疏导）。

### 2. 未来方向
当前MoMKE仍假设“训练阶段所有模态数据完整”，但现实中常存在训练数据不完整的情况（如部分场景无法采集视觉数据）。后续研究将聚焦“训练-测试双不完整”场景，探索无完整模态数据时的模态表示学习方法，进一步扩大框架的适用范围。

## 五、思考与启发
MoMKE的研究给多模态学习领域带来三点重要启示：
1. **回归基础价值**：在追求跨模态融合的同时，不应忽视单模态表示的判别性——基础能力扎实，融合才能更高效。
2. **分阶段设计的智慧**：“先让每个模块专精，再协同优化”的思路，可迁移到其他复杂任务（如多模态目标检测、跨模态翻译）。
3. **极端场景验证的必要性**：仅在完整模态场景下验证性能是不够的，聚焦单模态、少模态等极端场景，才能真正检验模型的鲁棒性。

相信随着MoMKE的进一步优化，以及更多研究者对“不完整多模态学习”的关注，未来的多模态系统将更能适应现实世界的复杂需求，为情感交互、人机协作等领域带来更优质的技术支撑。