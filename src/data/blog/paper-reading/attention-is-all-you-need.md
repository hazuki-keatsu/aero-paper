---
author: Hazuki Keatsu
tags:
  - 人工智能
  - 论文阅读
title: 《Attention is all you need》论文解读
pubDatetime: 2025-09-22T17:54:29+08:00
featured: false
draft: false
description: 《Attention is all you need》论文解读
---
## 前言

> [!TIP]
> 本文章有一个通俗版本，请看[Transformer 学习笔记](./transformer)

## 一、概述

在 Transformer 框架诞生之前，人们主要通过 CNN 和 RNN(相比于前者，这个被应用得更多一点) 来作为**序列转导模型**来对诸如自然语言等序列化的数据进行序列转导处理。这些当时主流模型具有以下特征：
1. 基于 RNN / CNN
2. 使用编码器-解码器的结构
3. 使用注意力机制增强表现

Transformer 创新点在于它完全摒弃了 RNN / CNN，同时完全基于注意力机制，但是它仍然在使用编码器-解码器的架构。通过以上的这些改进，使 Transformer 模型获得更好的表现的同时，并行计算的优势也极大的加快了模型训练的速度（当然，相比 RNN 推理速度是下降了的）。

> [!NOTE]
> **序列转导模型**指的使处理序列数据的模型。序列数据，即具有顺序关系的数据，每个元素的顺序对于数据的整体含义非常重要。
> 
> 典型的序列转导任务有：文本翻译、文本生成、语音转文字等。

## 二、前备知识

### 1. FNN(Feedforward Neural Network) 前馈神经网络

前馈神经网络由于它的输入层的长度是固定的，虽然可以通过平均或者拼接词嵌入来输入输入层，但是这样会导致序列信息的丧失，因此不太适合用在序列转导任务上。

![FNN 架构图](/blog-assets/FNN.svg)

### 2. RNN(Recurrent Neural Network) 循环神经网络

相比于前面 FNN，它解决了以下问题：

1. 能够建模词序：RNN 是按照时间顺序（token 顺序）逐个输入处理的
2. 能够建模上下文依赖：RNN 是逐个喂入词语的，并且会有“记忆”机制
3. 支持不定长度的输入：不再需要 FNN 那种固定长度的输入格式；

但是，由于循环的不断延长，它会导致以前输入的信息逐渐丢失。同时，RNN 只适合用来处理输入和输出长度一致的序列。

![RNN 架构图](/blog-assets/RNN.svg)

### 3. Encoder-decoder 编码器-解码器结构

让两个 RNN 单独处理输入和输出，输入的部分就是编码器，它会产生一个上下文向量，涵盖了整个输入文本的语义信息。解码器对上下文向量进行解码，将其转化为目标的输出。

![Encoder-decoder 架构图](/blog-assets/Encoder-decoder.svg)

### 4. Attention Mechanism 注意力机制

注意力机制解决的问题：
1. 解决模型处理长序列时的“遗忘”问题：随着序列长度的增长，远距离依赖信息在传递过程中易被稀释，导致模型对长距离依赖关系的建模能力减弱。
2. 解决不同时间步输入对当前时刻输出的“重要性”问题：所有时间步的输入在计算当前时刻输出时被同等对待，忽略了不同时间步对当前时刻输出的重要性可能存在的差异。

![Attention Mechanism](/blog-assets/SelfAttention.svg)

> [!TIP]
> 其实讲到这里，你会发现很多在序列转导任务的问题都已经被解决了。但是还有一个问题始终困扰着人们，那就是**并行化**计算的问题。由于 RNN 下一次生成的结果会依赖于上一次计算的结果，所以导致运算效率较低，因此论文中完全抛弃了 RNN 的架构。


## 三、Transformer 架构解析

为了解决并行计算的问题，有科学家提出使用 CNN 来替代 RNN 进行模型转导任务，但是这样做又将之前的 RNN 的记忆力下降的问题重新引回来了。在这种情况下，论文大胆提出 Transformer 架构——完全基于注意力机制。

<img class="prose" style="width: 50%" src="/blog-assets/ModalNet-21.png">

**架构解读：**

> [!TIP]
> [架构详解图](/blog-assets/TransformerModalStructure.svg)
> 此图片是svg矢量图，直接展示影响网站性能，所以请右键查看高清无损图片。


## 四、模型细节

上面的架构详解图已经对于模型整体的架构有了一个完善的介绍了，接下来主要是一些细节的地方的讲解。

### 1. 兼容性函数（compatibility function）

在注意力机制的语境中，这句话里的 “兼容性函数（compatibility function）” 可以理解为**衡量 “查询（Query）” 与 “键（Key）” 之间关联强度的计算方式**，其核心作用是为每个 “键 - 值对” 生成一个 “匹配分数”，最终用于确定 “值（Value）” 的加权权重。

**具体理解：**

1. **本质：相似度计算工具**

兼容性函数本质上是一种**相似度度量方法**，它接收两个向量 —— 查询（Q）和某个键（K），输出一个标量（数值），这个数值越大，表示 Q 和 K 的 “兼容性” 越高（即关联越紧密）。

例如：在机器翻译中，若 “查询” 是目标语言的某个词向量，“键” 是源语言的各个词向量，兼容性函数就会计算目标词与每个源词的关联强度（比如 “苹果” 作为查询时，与源语言中 “apple” 的兼容性会高于与 “book” 的兼容性）。

2. **常见实现方式**

兼容性函数的具体计算方式有多种，常见的包括：

- **点积（Dot Product）**：直接计算 Q 和 K 的内积（$Q\cdot K = \sum(Q_i \times K_i)$），是 Transformer 中使用的 “缩放点积注意力” 的基础；
- **加性函数（Additive Function）**：通过线性变换将 Q 和 K 映射到同一空间后相加，再经过非线性激活（如 $v^T \cdot tanh(WQ+UK)$）；
- **余弦相似度**：归一化后的向量夹角余弦值，专注于方向而非量级的匹配；
- **带参数的线性变换**：如通过一个小型神经网络学习 Q 和 K 的匹配模式。

3. **与注意力权重的关系**

兼容性函数的输出（每个 Q 与 K 的匹配分数）并非直接作为权重，而是需要经过**softmax 归一化**处理，转化为总和为 1 的权重分布。例如：

- 假设兼容性函数计算出 3 个键的分数为 [3, 1, 2]；
- 经 softmax 后得到权重 [0.66, 0.09, 0.25]；
- 最终输出是这 3 个键对应的值（V1, V2, V3）的加权和：0.66×V1 + 0.09×V2 + 0.25×V3。

### 2. 共享参数

#### 1) 自注意力（Self-Attention）：完全共享

自注意力的核心是 “输入序列对自身建模关联”，因此：

- 提取 $K$ 的转换矩阵$W_K$和提取 $Q$ 的转换矩阵$W_Q$，均是**基于同一输入矩阵学习的独立参数**，但二者针对 “输入→K” 和 “输入→Q” 的映射任务各自优化（$W_K≠W_Q$）；

- 所谓 “从上下文矩阵提取 $K$/$Q$”，本质是 “输入经$W_K$/$W_Q$转换后形成上下文 $K$/$Q$ 矩阵”——**转换矩阵本身就是连接输入与上下文的唯一参数**，不存在 “两个不同转换矩阵”，自然不存在 “是否共享” 的问题（自身与自身必然共享）。

#### 2) 交叉注意力（Cross-Attention）：源 - 目标不共享，内部绑定

在 Decoder 的交叉注意力模块（如机器翻译中 Decoder 关注 Encoder 输出）中：

- **Q 的转换矩阵**：基于 Decoder 自身的输入（前一层隐藏态）学习，记为$W_Q^{dec}$；

- **K 的转换矩阵**：基于 Encoder 的输出（上下文矩阵）学习，记为$W_K^{enc}$；

- 此时 “Decoder 输入→Q” 与 “Encoder 上下文→K” 的转换矩阵**不共享**（因数据源和任务不同），但各自内部仍遵循 “转换矩阵与输入强绑定” 的规则 ——$W_K^{enc}$就是 Encoder 侧从输入到 K 的唯一转换矩阵，$W_Q^{dec}$是 Decoder 侧从输入到 Q 的唯一转换矩阵。

**效率优化变体中的特殊情况：**

随着模型规模扩大，为平衡效率与性能，出现了参数共享的变体设计，但核心逻辑仍是 “绑定输入与转换矩阵”：

#### 3) 多头注意力（MHA）：头内独立，头间不共享

标准多头注意力中：

- 输入矩阵先经$W_Q$（维度$d_\text{hidden size}×d_\text{hidden size}$）转换为 $Q$ 矩阵，再拆分为$h$个独立的头级 $Q$ 向量；

- 每个头的 $Q$ 向量本质是$W_Q$中对应子矩阵的映射结果，**不同头的 $K$/$Q$ 转换子矩阵不共享**（保证头间特征多样性），但每个头的转换子矩阵仍与输入强绑定（无额外上下文提取矩阵）。

#### 4) 多查询注意力（MQA）：KV 全局共享，Q 独立

为减少 KV 缓存内存开销，MQA 设计为：

- $Q$ 保持多头独立：输入经$W_Q$（$d_\text{hidden size}×d_\text{hidden size}$）转换，拆分为多个头；

- $K$/$V$ 全局共享：所有头共享一组$W_K$/$W_V$（维度$d_\text{hidden size}×d_\text{head}$），输入经该矩阵转换后广播到所有头；

- 此处$W_K$仍是 “输入→K” 的唯一转换矩阵，共享的是 “头间参数”，而非 “输入与上下文的转换矩阵”—— 本质是参数复用，而非额外矩阵。

#### 5) 分组查询注意力（GQA）：KV 分组共享

介于 MHA 与 MQA 之间的折中方案：

- 输入经$W_K$（$d_\text{hidden size}×(h×d_\text{head})$）转换为 $K$ 矩阵，按组分配给多个 $Q$ 头；

- 每组 $Q$ 头共享同一组$W_K$子矩阵，但$W_K$本身仍是输入到 $K$ 的唯一转换矩阵，无额外上下文提取参数。

#### 6) 关键结论与对比

| 注意力类型     | 转换矩阵与输入的关系         | 参数共享情况                  | 典型场景                   |
| --------- | ------------------ | ----------------------- | ---------------------- |
| 标准自注意力    | 转换矩阵是输入到 K/Q 的唯一桥梁 | 完全共享（同一矩阵连接输入与上下文）      | Encoder 层、Decoder 自注意力 |
| 交叉注意力     | 源 / 目标侧转换矩阵各自绑定输入  | 源 - 目标不共享，各自内部共享        | Decoder-Encoder 交互     |
| MQA（效率优化） | K/V 转换矩阵绑定输入       | 所有头共享同一$W_K$/$W_V$，Q 独立 | 高并发推理、边缘部署             |
| GQA（平衡方案） | K/V 转换矩阵绑定输入       | 同组头共享$W_K$/$W_V$子矩阵     | 通用大模型                  |

## 五、训练细节

### 1. 优化器

我们采用 Adam 优化器，参数设置为 β₁=0.9、β₂=0.98、ε=10⁻⁹。训练过程中，学习率按照以下公式动态调整：

$lrate=d_{model}^{-0.5}\cdot \operatorname* {min}(step\_ num^{-0.5},step\_ num\cdot warmup\_steps^{-1.5})$

该公式表示：在初始的 $warmup_steps$ 训练步骤内，学习率呈线性增长；之后，学习率与步骤数的平方根成反比衰减。其中，$warmup_steps$ 设置为 4000。

### 2. 正则化

训练过程中，我们采用了三种正则化方法：

1. **残差丢弃**：在每个子层的输出结果上应用 dropout，随后再与子层输入相加并进行归一化。此外，在编码器和解码器的嵌入层与位置编码的求和结果上也应用了 dropout。基础模型的丢弃率设置为 0.1。

2. **标签平滑**：训练时采用标签平滑技术，平滑参数 εₗₛ=0.1。这一操作会提高模型的困惑度（Perplexity），因为模型的预测会变得更不确定，但能够提升模型的准确率和 BLEU 分数。

## 六、总结

《Attention is all you need》论文以序列转导任务的主流方案（RNN/CNN+注意力）为出发点，指出它们受限于串行计算和长距离依赖问题，随后引出 Transformer 架构完全摒弃循环与卷积，仅依赖多头自注意力与编码器 - 解码器结构来实现高效并行建模。

文章先补充 FNN、RNN、编码器-解码器以及注意力机制等基础概念，强调注意力在保留序列信息、衡量 Query-Key 关联中的作用，并详细解释兼容性函数的多种实现及 softmax 归一化带来的权重分布。围绕 Transformer 的细节，作者讲解了从自注意力、交叉注意力到多头/多查询/分组查询等变体讨论参数共享策略及其效率权衡。训练部分则介绍 Adam 优化器配合 warmup 策略的学习率调度，同时说明残差 dropout 与标签平滑等正则化手段如何提升泛化与 BLEU 表现。整体文章结构循序渐进，让读者既理解 Transformer 抛弃 RNN/CNN 的原因，也掌握其核心机制和训练技巧。
