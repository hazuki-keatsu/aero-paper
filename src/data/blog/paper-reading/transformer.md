---
author: Hazuki Keatsu
tags:
  - 人工智能
title: Transformer 学习笔记
pubDatetime: 2025-09-15T17:52:18+08:00
featured: false
draft: false
description: 李宏毅老师的机器学习相关课程中 Transformer 的学习笔记。
modDatetime: 2025-09-19T23:32:38+08:00
---
## 前言

Transformer 是一个序列向序列转换的模型。通常来说，输出的长度与输入的长度是不相等的。

语音识别、语音翻译、机器翻译、对象侦测等行为都可以视为 Transformer 的使用场景。

> [!WARNING]
> 这篇文章正在续写中......

## 一、概述

在 Transformer 框架诞生之前，人们主要通过 CNN 和 RNN(相比于前者，这个被应用得更多一点) 来作为**序列转导模型**来对诸如自然语言等序列化的数据进行序列转导处理。这些当时主流模型具有以下特征：
1. 基于 RNN / CNN
2. 使用编码器-解码器的结构
3. 使用注意力机制增强表现

Transformer 创新点在于它完全摒弃了 RNN / CNN，同时完全基于注意力机制，但是它仍然在使用编码器-解码器的架构。通过以上的这些改进，使 Transformer 模型获得更好的表现的同时，并行计算的优势也极大的加快了模型训练的速度（当然，相比 RNN 推理速度是下降了的）。

> [!NOTE]
> **序列转导模型**指的使处理序列数据的模型。序列数据，即具有顺序关系的数据，每个元素的顺序对于数据的整体含义非常重要。
> 
> 典型的序列转导任务有：文本翻译、文本生成、语音转文字等。

## 二、前备知识

### 1. FNN(Feedforward Neural Network) 前馈神经网络

前馈神经网络由于它的输入层的长度是固定的，虽然可以通过平均或者拼接词嵌入来输入输入层，但是这样会导致序列信息的丧失，因此不太适合用在序列转导任务上。

![FNN 架构图](/blog-assets/FNN.svg)

### 2. RNN(Recurrent Neural Network) 循环神经网络

相比于前面 FNN，它解决了以下问题：

1. 能够建模词序：RNN 是按照时间顺序（token 顺序）逐个输入处理的
2. 能够建模上下文依赖：RNN 是逐个喂入词语的，并且会有“记忆”机制
3. 支持不定长度的输入：不再需要 FNN 那种固定长度的输入格式；

但是，由于循环的不断延长，它会导致以前输入的信息逐渐丢失。同时，RNN 只适合用来处理输入和输出长度一致的序列。

![RNN 架构图](/blog-assets/RNN.svg)

### 3. Encoder-decoder 编码器-解码器结构

让两个 RNN 单独处理输入和输出，输入的部分就是编码器，它会产生一个上下文向量，涵盖了整个输入文本的语义信息。解码器对上下文向量进行解码，将其转化为目标的输出。

![Encoder-decoder 架构图](/blog-assets/Encoder-decoder.svg)

### 4. Attention Mechanism 注意力机制

注意力机制解决的问题：
1. 解决模型处理长序列时的“遗忘”问题：随着序列长度的增长，远距离依赖信息在传递过程中易被稀释，导致模型对长距离依赖关系的建模能力减弱。
2. 解决不同时间步输入对当前时刻输出的“重要性”问题：所有时间步的输入在计算当前时刻输出时被同等对待，忽略了不同时间步对当前时刻输出的重要性可能存在的差异。



## 一、主要的处理流程

### 1. 整体架构

![](/blog-assets/transformer-overview-structure.svg)

### 2. 架构细节

![](/blog-assets/ModalNet-21.png)

在 Transformer 的内部结构中，有很多重复的结构，每一个这样的结构都会被称为一个 **Block**。

### 3. 工作流程

以文本翻译场景为例（本来 Transformer 诞生的目的就是为了提升 Google 翻译的精确性）：

Transformer模型的工作流程可以分为**编码器（Encoder）处理**、**解码器（Decoder）处理**和**输出生成**三个核心阶段，整体遵循序列到序列的转换逻辑。
#### 一、输入预处理

在进入编码器之前，需要对原始文本进行预处理： 

1. **分词（Tokenization）** 将输入文本拆分为最小语义单位（如单词、子词或字符），每个单位称为一个`token`。例如，"I love machine learning"可能被拆分为`["I", "love", "machine", "learning"]`。 
2. **嵌入（Embedding）** 每个`token`通过嵌入层转换为固定维度的向量（如512维），捕捉基础语义信息。这一步将离散的文本符号映射到连续的向量空间。 
3. **位置编码（Positional Encoding）** 由于Transformer没有循环结构，需手动加入位置信息。通过正弦或者余弦函数生成位置向量，与词嵌入向量相加，使模型感知token的顺序关系。 

#### 二、编码器（Encoder）处理

编码器的作用是将输入序列转换为包含上下文信息的**上下文向量（Context Vector）**，由N个相同的Encoder Block堆叠而成。

每个Encoder Block包含两个核心模块： 
1. **多头自注意力（Multi-Head Self-Attention）** 
	- 对输入序列中的每个token，计算它与其他所有token的关联程度（注意力权重）。例如，在”猫追狗，它跑得很快“中，模型通过注意力机制判断”它“指代”猫“还是”狗“。
	- ”多头“意味着将注意力机制分成多个并行头，分别捕捉不同类型的关联（如语法依赖、语义关联），最后拼接结果。 
2. **前馈神经网络（Feed-Forward Network, FFN）** 对每个token的注意力输出进行独立的非线性变换（先升维再降维），增强模型对局部特征的捕捉能力。

**补充机制**： 
- 每个模块后都有**残差连接**（输入+输出），避免深层网络的梯度消失。 
- 每个残差连接后都有**层归一化（Layer Normalization）**，稳定训练时的数值分布。 

> [!NOTE]
> 1. 残差连接：一种跳过网络中部分层的连接方式，核心是将某一层的输入直接与该层的输出相加，形成 “输入 + 输出” 的残差结构。
> 2. 层归一化：层归一化是一种数据标准化技术，通过对**单个样本的所有特征维度**进行归一化，使数据分布满足 “均值为 0，方差为 1”

#### 三、解码器（Decoder）处理 

解码器的作用是基于编码器输出的上下文向量，生成目标序列，同样由N个相同的Decoder Block堆叠而成。 
每个Decoder Block包含三个核心模块： 
1. **掩码多头自注意力（Masked Multi-Head Self-Attention）** 
	与编码器的自注意力类似，但加入了**掩码（Mask）**，确保生成第i个token时，只能看到前i-1个已生成的token（避免”偷看“未来信息）。例如，生成翻译句子时，不会提前利用后面的词。 
2. **编码器-解码器注意力（Encoder-Decoder Attention）** 
	以解码器的输出为”查询（Query）“，编码器的输出为“键（Key）”和“值（Value）”，让解码器关注输入序列中与当前生成内容相关的部分。例如，翻译时，解码器生成"苹果"时会重点关注输入中的”apple“。
3. **前馈神经网络（FFN）** 与编码器的FFN功能相同，对注意力输出进行非线性变换。 

**补充机制**： 同样包含残差连接和层归一化，确保深层网络的稳定性。

#### 四、输出生成 

解码器的最终输出通过一个线性层和softmax函数转换为目标序列的概率分布： 

1. **线性层**：将解码器输出的高维向量映射到目标词汇表的维度。 
2. **softmax**：将向量转换为概率分布，每个位置对应一个token的生成概率。 
3. **自回归生成**： 每次选取概率最高的 token 作为当前输出，然后将其作为新的输入反馈到解码器，重复这一过程，直到生成特殊的“终止符（END）”。 

#### 总结：端到端流程示例（以机器翻译为例）

1. 输入：英文句子 “I love AI”
2. 预处理：分词→嵌入→加位置编码，得到输入向量序列。 
3. 编码器：通过多层自注意力和FFN，生成包含上下文的向量序列。 
4. 解码器： 
	- 初始输入为“起始符（BEGIN）”，通过掩码自注意力关注已生成内容（初始为空）。 
	- 通过编码器-解码器注意力关联英文输入的上下文向量。 
	- 生成第一个中文token”我“。 
5. 迭代生成：将“我”作为新输入，重复解码器步骤，依次生成“爱”、“人工智能”、“END”，最终输出“我爱人工智能”。 

Transformer 的核心优势在于**并行计算能力**（摆脱RNN的序列依赖）和**长距离依赖捕捉能力**（通过自注意力机制），这使其在机器翻译、文本生成等任务中表现优异。

